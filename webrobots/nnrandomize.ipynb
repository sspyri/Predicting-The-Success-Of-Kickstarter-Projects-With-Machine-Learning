{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   42.0s\n",
      "C:\\Users\\dungeon\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 353 tasks      | elapsed: 10.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dungeon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 16.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dungeon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "21408/21408 [==============================] - 7s 307us/step - loss: 0.6410 - acc: 0.6173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weight': 2,\n",
       " 'units': 64,\n",
       " 'optimizer': 'adam',\n",
       " 'nb_epoch': 59,\n",
       " 'kernel': 'random_normal',\n",
       " 'dropout_rate': 0,\n",
       " 'batch_size': 7}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\dungeon\\Desktop\\New folder\\Kainouria attempt\\Labeled_Minmaxed.csv')\n",
    "X = df.drop(columns=['backers_count','pledged','converted_pledged_amount'\n",
    "      ,'id','usd_pledged','state','static_usd_rate','funded_percentage'])\n",
    "y = df['state'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 42,stratify=y)\n",
    "\n",
    "batch_size = [int(x) for x in np.linspace(2,50,20)]\n",
    "nb_epoch = [int(x) for x in np.linspace(start =50,stop = 500,num=50)]\n",
    "optimizer = ['adam']\n",
    "kernel = ['random_normal']\n",
    "units = [4,8,16,32,64,128]\n",
    "dropout_rate = [int(x) for x in np.linspace(0.0,1,10)]\n",
    "weight = [int(x) for x in np.linspace (1,5,5)]\n",
    "parameters = {'batch_size': batch_size,\n",
    "             'nb_epoch': nb_epoch,\n",
    "             'optimizer': optimizer,\n",
    "             'kernel':kernel,\n",
    "             'units': units,\n",
    "              'dropout_rate':dropout_rate,\n",
    "              'weight': weight\n",
    "             }\n",
    "\n",
    "# K fold validation\n",
    "def build_classifier(optimizer, kernel, units,dropout_rate = 0.0,weight=0):\n",
    "    # Initializing Neural Network\n",
    "    classifier = Sequential()\n",
    "    # First Hidden Layer\n",
    "    classifier.add(Dense(units = units, activation = 'relu',kernel_constraint=maxnorm(weight),\n",
    "    kernel_initializer = kernel, input_dim = 10))\n",
    "    # Second Hidden Layer\n",
    "    classifier.add(Dense(units = units, activation='relu',kernel_constraint=maxnorm(weight),\n",
    "    kernel_initializer = kernel))\n",
    "    #dropout rate\n",
    "    classifier.add(Dropout(dropout_rate))\n",
    "    # Output Layer\n",
    "    classifier.add(Dense(1,activation='sigmoid',\n",
    "    kernel_initializer= kernel))\n",
    "    # Compiling NN\n",
    "    classifier.compile(optimizer=optimizer,loss = 'binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "  \n",
    "    return classifier\n",
    "\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator = classifier, param_distributions = parameters,n_iter= 100, cv =5, n_jobs = -1,random_state = 42,verbose = 2)\n",
    "\n",
    "random_search.fit(X_train,y_train)\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
